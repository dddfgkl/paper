# Reference  
文本分类  


- Bag of Tricks for Efficient Text Classification  
著名的FastText  
[LINK:] https://arxiv.org/abs/1607.01759  


- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
Bert是NLP通用模型的一个开始,Transformer大放异彩  
[LINK:] https://arxiv.org/abs/1810.04805   

- How to Fine-Tune BERT for Text Classification?  
如何去fine tune bert，如果文本很长采用什么策略来fine tune，Bert各个sentence embeding层都学习到了什么？
这个论文或许会给出一点建议  
[LINK:] https://arxiv.org/pdf/1905.05583.pdf  
[MyNote:](./How%20to%20Fine-Tune%20BERT%20for%20Text%20Classification.md)  


- Transformer-XL: Attentive Language Models   
XLNet使用的单元就是Transformer-XL  
[LINK:] https://arxiv.org/pdf/1901.02860.pdf  
[MyNote:](./Transformer-XL.md)   

- XLNet: Generalized Autoregressive Pretraining for Language Understanding  
XLNet对Bert无法处理过长的文本提出了一种解决思路，效果上来说也有所提升  
[LINK:] https://arxiv.org/abs/1906.08237  
[MyNote:](./XLNet.md)  





-     